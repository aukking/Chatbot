# -*- coding: utf-8 -*-
"""nlp243-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P4Op6mUPKgLNodvyjloSkqV8pCKIOMae
"""
from models import *
import numpy as np
import random

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

MESSAGE = data.Field(init_token='<sos>',
                     eos_token='<eos>',
                     lower=True)

REPLY = data.Field(init_token='<sos>',
                   eos_token='<eos>',
                   lower=True)

fields = ((None, None), ('message', MESSAGE), ('reply', REPLY))

# using the small_train_messages to help with training time
train_data, valid_data = data.TabularDataset.splits(
    path='data',
    train='small_train_messages.csv',
    validation='valid_messages.csv',
    format='csv',
    fields=fields,
    skip_header=True
)

MESSAGE.build_vocab(
    train_data,
    vectors='glove.6B.50d',
    unk_init=torch.Tensor.normal_
)

REPLY.build_vocab(
    train_data,
    vectors='glove.6B.50d',
    unk_init=torch.Tensor.normal_
)

# Q: Why are we building the vocab twice?
# MESSAGE.build_vocab(train_data, min_freq=2)
# REPLY.build_vocab(train_data, min_freq=2)

BATCH_SIZE = 64

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator, valid_iterator = data.BucketIterator.splits(
    (train_data, valid_data),
    batch_size=BATCH_SIZE,
    sort=False,
    device=device)

INPUT_DIM = len(MESSAGE.vocab)
OUTPUT_DIM = len(REPLY.vocab)
ENC_EMBEDDING_DIM = 50
DEC_EMBEDDING_DIM = 50
HIDDEN_DIM = 50
N_LAYERS = 2
ENC_DROPOUT = 0.25
DEC_DROPOUT = 0.25
REPLY_PAD_IDX = REPLY.vocab.stoi[REPLY.pad_token]

# variabel used to toggle if we want to train or run the model
TRAIN = False

enc = Encoder(INPUT_DIM, ENC_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)

model = Seq2Seq(enc, dec, device).to(device)
print(f'The model has {count_parameters(model):,} trainable parameters')

MESSAGE_PAD_IDX = REPLY.vocab.stoi[REPLY.pad_token]
criterion = nn.CrossEntropyLoss(ignore_index=MESSAGE_PAD_IDX)
optimizer = optim.Adam(model.parameters())

if TRAIN:
    model.apply(init_weights)
    # pretrained_embeddings = MESSAGE.vocab.vectors
    lstm_trainer = Trainer(model, optimizer, criterion, device=device)

    lstm_trainer.run_training(train_iterator, valid_iterator, REPLY.vocab.stoi, n_epochs=10)

    torch.save(model.state_dict(), 'first_mode.pt')
else:
    model.load_state_dict(torch.load('first_mode.pt'))
    lstm_trainer = Trainer(model, optimizer, criterion, device=device)

    # first step to predict is to vectorize a message
    message = 'hello world'
    message_embeddings = MESSAGE.vocab.vectors
    str2idx = MESSAGE.vocab.stoi
    vectorized_input = []
    for word in message.split():
        vectorized_input.append(message_embeddings[str2idx[word]].unsqueeze(0))

    tensor_input = torch.cat(vectorized_input).unsqueeze(0)
    prediction = lstm_trainer.predict_raw(tensor_input)
    print(prediction)




