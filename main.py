# -*- coding: utf-8 -*-
"""nlp243-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P4Op6mUPKgLNodvyjloSkqV8pCKIOMae
"""
from models import *
import numpy as np
import random

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

MESSAGE = data.Field(init_token='<sos>',
                     eos_token='<eos>',
                     lower=True)

REPLY = data.Field(init_token='<sos>',
                   eos_token='<eos>',
                   lower=True)

fields = ((None, None), ('message', MESSAGE), ('reply', REPLY))

# using the small_train_messages to help with training time
train_data, valid_data = data.TabularDataset.splits(
    path='data',
    train='small_train_messages.csv',
    validation='valid_messages.csv',
    format='csv',
    fields=fields,
    skip_header=True
)

UNK_THRESH = 3

MESSAGE.build_vocab(
    train_data,
    vectors='glove.6B.50d',
    min_freq = UNK_THRESH,
    unk_init=torch.Tensor.normal_
)

REPLY.build_vocab(
    train_data,
    vectors='glove.6B.50d',
    min_freq = UNK_THRESH,
    unk_init=torch.Tensor.normal_
)

# 64 seems to work for 14 million parameters on 50,000 lines of training input
# 100 doesn't work for the above mentioned parameters
# 75 starts fine
BATCH_SIZE = 128

# variable used to toggle if we want to train or run the model
TRAIN = False

if TRAIN:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
else:
    # use the CPU when running the model
    device = 'cpu'

train_iterator, valid_iterator = data.BucketIterator.splits(
    (train_data, valid_data),
    batch_size=BATCH_SIZE,
    sort_key=lambda x: len(x.message),
    sort=True,
    device=device)

INPUT_DIM = len(MESSAGE.vocab)
OUTPUT_DIM = len(REPLY.vocab)
ENC_EMBEDDING_DIM = 50
DEC_EMBEDDING_DIM = 50
HIDDEN_DIM = 50
N_LAYERS = 2
ENC_DROPOUT = 0.25
DEC_DROPOUT = 0.25
PAD_IDX = REPLY.vocab.stoi[REPLY.pad_token]
SOS = REPLY.vocab.stoi['<sos>']
EOS = REPLY.vocab.stoi['<eos>']

attn = Attention(HIDDEN_DIM)
enc = Encoder(INPUT_DIM, ENC_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT, attn)

# # Initialize model embedding layer
pretrained_embeddings = MESSAGE.vocab.vectors
enc.embedding.weight.data.copy_(pretrained_embeddings)
enc.embedding.weight.data[PAD_IDX] = torch.zeros(50)

pretrained_embeddings = REPLY.vocab.vectors
dec.embedding.weight.data.copy_(pretrained_embeddings)
dec.embedding.weight.data[PAD_IDX] = torch.zeros(50)

model = Seq2SeqBeam(enc, dec, device, sos=SOS, eos=EOS).to(device)

print(f'The model has {count_parameters(model):,} trainable parameters')

criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = optim.Adam(model.parameters())

if TRAIN:
    model.apply(init_weights)

    lstm_trainer = Trainer(model, optimizer, criterion, device=device)

    lstm_trainer.run_training(train_iterator, valid_iterator, n_epochs=40)

    torch.save(model.state_dict(), 'attn_big_lowunk.pt')

else:
    model.load_state_dict(torch.load('attn_big_lowunk.pt'))
    lstm_trainer = Trainer(model, optimizer, criterion, device=device)

    # first step to predict is to vectorize a message
    while True:
        message = input('')

        tensor_input, unks = vectorize_input(message, MESSAGE)

        # we would like these predictions to be softmaxed
        prediction = lstm_trainer.predict(tensor_input)
        print(decode_prediction_beam(prediction, REPLY))




