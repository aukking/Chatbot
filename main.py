# -*- coding: utf-8 -*-
"""nlp243-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P4Op6mUPKgLNodvyjloSkqV8pCKIOMae
"""
from models import *
import numpy as np
import random
from sklearn.metrics import classification_report

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

MESSAGE = data.Field(init_token='<sos>',
                     eos_token='<eos>',
                     lower=True)

REPLY = data.Field(init_token='<sos>',
                   eos_token='<eos>',
                   lower=True)

fields = ((None, None), ('message', MESSAGE), ('reply', REPLY))

# using the small_train_messages to help with training time
train_data, valid_data = data.TabularDataset.splits(
    path='data',
    train='small_train_messages.csv',
    validation='valid_messages.csv',
    format='csv',
    fields=fields,
    skip_header=True
)

MESSAGE.build_vocab(
    train_data,
    vectors='glove.6B.50d',
    unk_init=torch.Tensor.normal_
)

REPLY.build_vocab(
    train_data,
    vectors='glove.6B.50d',
    unk_init=torch.Tensor.normal_
)

# max batch size allowable on Austin's machine: 128
# important, need to note what batch size each model runs on
# second_big_model.pt runs on 16
# first_big_model.pt runs on ???
BATCH_SIZE = 64

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator, valid_iterator = data.BucketIterator.splits(
    (train_data, valid_data),
    batch_size=(BATCH_SIZE, 1),
    sort_key=lambda x: len(x.message),
    sort_within_batch=False,
    device=device)

INPUT_DIM = len(MESSAGE.vocab)
OUTPUT_DIM = len(REPLY.vocab)
ENC_EMBEDDING_DIM = 50
DEC_EMBEDDING_DIM = 50
HIDDEN_DIM = 50
N_LAYERS = 2
ENC_DROPOUT = 0.25
DEC_DROPOUT = 0.25
PAD_IDX = REPLY.vocab.stoi[REPLY.pad_token]

# variable used to toggle if we want to train or run the model
TRAIN = False

enc = Encoder(INPUT_DIM, ENC_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)

# # Initialize model embedding layer
pretrained_embeddings = MESSAGE.vocab.vectors
enc.embedding.weight.data.copy_(pretrained_embeddings)
enc.embedding.weight.data[PAD_IDX] = torch.zeros(50)

pretrained_embeddings = REPLY.vocab.vectors
dec.embedding.weight.data.copy_(pretrained_embeddings)
dec.embedding.weight.data[PAD_IDX] = torch.zeros(50)

model = Seq2Seq(enc, dec, device).to(device)

# why do we do this twice in the thing?
# model.apply(init_weights)

print(f'The model has {count_parameters(model):,} trainable parameters')

criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = optim.Adam(model.parameters())

if TRAIN:
    model.apply(init_weights)

    lstm_trainer = Trainer(model, optimizer, criterion, device=device)

    lstm_trainer.run_training(train_iterator, valid_iterator, n_epochs=10)

    torch.save(model.state_dict(), 'second_big_model.pt')

else:
    model.load_state_dict(torch.load('first_big_model.pt'))
    lstm_trainer = Trainer(model, optimizer, criterion, device=device)

    predictions = []
    ground_truths = []
    for item in valid_iterator:
        print(item)
        pred = lstm_trainer.predict(item.message)
        predictions.append(pred)
        ground_truths.append(item.reply)
    print(classification_report(ground_truths, predictions))


    # first step to predict is to vectorize a message
    while True:
        message = input('')

        tensor_input, unks = vectorize_input(message, MESSAGE)

        prediction = lstm_trainer.predict(tensor_input)

        print(decode_prediction(prediction, REPLY))




